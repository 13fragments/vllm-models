# syntax=docker/dockerfile:1.7
ARG BASE_IMAGE=vllm/vllm-openai:v0.6.0
FROM ${BASE_IMAGE}

# Build arguments
ARG MODEL_ID
ARG MODEL_DIR=/models/model
ARG HF_CACHE=/root/.cache/huggingface
ARG SERVE_ARGS="--host 0.0.0.0 --port 8000"

# Environment variables
ENV HF_HOME=${HF_CACHE} \
    HUGGINGFACE_HUB_CACHE=${HF_CACHE} \
    MODEL_ID=${MODEL_ID} \
    MODEL_DIR=${MODEL_DIR}

# Download model weights during build (with cache mount for efficiency)
RUN --mount=type=cache,target=/root/.cache/huggingface \
    mkdir -p ${MODEL_DIR} && \
    huggingface-cli download "${MODEL_ID}" \
        --local-dir "${MODEL_DIR}" \
        --local-dir-use-symlinks False

# Copy licenses (generated by harvest_licenses.py in CI, or placeholder README in repo)
COPY licenses/ /licenses/

# OCI labels (placeholders - will be set by docker build --label)
LABEL org.opencontainers.image.title="vLLM Fat Image"
LABEL org.opencontainers.image.description="vLLM image with embedded model weights"
LABEL org.opencontainers.image.source="https://github.com/13fragments/vllm-models"

# Expose default vLLM port
EXPOSE 8000

# Direct command to start vLLM with embedded model
CMD ["sh", "-c", "vllm serve ${MODEL_DIR} ${SERVE_ARGS}"]