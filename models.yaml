defaults:
  base_image: "vllm/vllm-openai:v0.6.0"
  arch: "linux/amd64"
  download_root: "/models"
  hf_cache_dir: "/root/.cache/huggingface"
  spdx_permissive_whitelist:
    - apache-2.0
    - mit
    - bsd-2-clause
    - bsd-3-clause
    - isc
    - cc0-1.0
    - unlicense
  publish:
    ghcr: true
    dockerhub: false

models:
  - id: "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
    short: "deepseek-r1-distill-qwen-15b"
    revision: null
    permissive: false  # Force slim-only to avoid disk space issues on GH Actions
    override_spdx: null
    gated: auto
    serve_args: ["--host", "0.0.0.0", "--port", "8000"]
    oci:
      title: "vLLM + DeepSeek-R1-Distill-Qwen-1.5B"
      description: "Slim/Fat images built from Hugging Face; see /licenses for compliance."
      url: "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
    publish:
      ghcr: true
      dockerhub: false

  - id: "NousResearch/Hermes-4-70B"
    short: "hermes-4-70b"
    revision: null
    permissive: false  # Force slim-only for large 70B model
    override_spdx: null
    gated: auto
    serve_args: ["--host", "0.0.0.0", "--port", "8000"]
    oci:
      title: "vLLM + Hermes-4-70B"
      description: "Slim/Fat images built from Hugging Face; see /licenses for compliance."
      url: "https://huggingface.co/NousResearch/Hermes-4-70B"
    publish:
      ghcr: true
      dockerhub: false

  - id: "cpatonn/Hermes-4-70B-AWQ-4bit"
    short: "hermes-4-70b-awq-4bit"
    revision: null
    permissive: false  # Force slim-only for quantized 70B model
    override_spdx: null
    gated: auto
    serve_args: ["--host", "0.0.0.0", "--port", "8000", "--quantization", "awq"]
    oci:
      title: "vLLM + Hermes-4-70B-AWQ-4bit"
      description: "Slim image with AWQ 4-bit quantization; see /licenses for compliance."
      url: "https://huggingface.co/cpatonn/Hermes-4-70B-AWQ-4bit"
    publish:
      ghcr: true
      dockerhub: false